Hand in 1 - 2024 Machine Learning
For the first hand in you will implement Logistic Regression and Softmax Regression for classification. The descriptions below describe what you are meant to do and hand in.

Start Early, Use The Study Cafe, Check Your Shapes, try and deal with numerial issues

Logistic Regression
Implementing Logistic Regression
In this exercise you must implement logistic regression and test it on text classification. We have provided starter code in the file logistic_regression.py. Here you should complete the following methods to implement a Logistic Regression Classifier.

logistic
predict
score
cost_grad
fit
where predict, score, cost_grad, fit are class methods of the classifier you must implement. The interface for each function is described in the file.

All needed equations can be found in the slides.

You can test your implementation by running python logistic_regression.py. This is a small non-exhaustive test. You should consider writing your own test cases.

Cost and gradient computations sometimes suffer from numerical issues if we are not careful. Exponentation of large numbers and log of small numbers can lead to numerical issues. It is possible to implement the algorithm to be more numerically stable if you do not compute numbers you do not need. The algorithm should work well enough even if you are not so careful and it is not a requirement for passing the hand-in to make the ultimate numerically stable algorithm.

Applying Logistic Regression
You will test your implementation on a real-life data set obtained from the danish central business register (CVR). The data set consists of text descriptions of the purpose of a company, such as:

"Selskabets form√•l er at drive restaurant og dertil knyttet virksomhed."

together with a label indicating the so-called industry code, e.g. 561010 is the code for "Restauranter" and 620100 is the code for "Computerprogrammering".

We have already written code that transforms a text description into a feature vector, so you do not need to worry about this translation. We will see later in the course how to do so. In this exercise, you will train a logistic regression classifier to classify descriptions of company purposes into the two classes "Restauranter" and "Computerprogrammering". If you want to have a look at the data, you can unzip "branchekoder_formal.gzip" and open it as a text file (keep the .gzip file in the directory). We are only extracting companies falling in the two categories 561010 and 620100.

Run python logistic_test.py and see your in-sample and test accuracy on text classification of industry codes (real data)

The code automatically saves the generated plot to include in your report. With a correct implementation and setting of learning rate, batch_size, epochs you should get above 95 percent test accuracy.

Report
Add a section called "PART I: Logistic Regression" with subsections "Code" and "Theory" to your report. In the code subsection you should have the following subsubsections

Summary and Results: Include the plot generated by logistic_test and include the in-sample and test accuracy you achieve. Add at most two lines explaining the plot(s) and comment anything you believe sticks out. Explain if anything does not work.
Actual Code: Include in your handin code snippets cost_grad and fit (using for instance verbatim environment in latex)
Furthermore you must answer the following three theoretical questions

Theoretical Questions
What is the running time of your mini-batch gradient descent algorithm?
The parameters:

n: number of training samples
d: dimensionality of training samples
epochs: number of epochs run
mini_batch_size: batch_size for mini_batch_gradient_descent
Write both the time to compute the cost and the gradient for log_cost You can assume that multiplying an 
 matrix with a 
 matrix takes 
 time.

Sanity Check:
Assume you are using Logistic Regression for classifying images of cats and dogs. What happens if we randomly permute the pixels in each image (with the same permutation) before we train the classifier? Will we get a classifier that is better, worse, or the same than if we used the raw data? Give a short explanation (at most three sentences).

Linearly Separable Data:
If the data is linearly separable, what happens to weights when we implement logistic regression with gradient descent? That is, how do the weights that minimize the negative log likelihood look like? You may assume that we have full precision (that is, ignore floating point errors) and we can run gradient descent as long as we want (i.e. what happens with the weights in the limit).

Give a short explanation for your answer. You may include math if it helps (at most 5 lines).

Softmax Regression
Implementing Softmax
In "softmax.py" you must complete the implementation of

softmax
predict
score
cost_grad
fit
The interface for each function is described in the file. All needed equations can be found in the slides and in the softmax notebook. Note that we have added a helper function def one_in_k_encoding(vec, k) that encodes a vector og length 
 of integer labels with class labels in 
 to a 
 matrix of one-in-k encoded labels.

You can test your implementation by running "python softmax.py". This is a small non-exhaustive test. You should consider writing your own test cases.

As for Logistic Regression, softmax sometimes suffers from numerical issues if we are not careful. Exponentation of large numbers and log of small numbers can lead to numerical issues. It is possible to implement the algorithm to be quite numerically stable if you use the trick provided in the Softmax note under the course reading material for Softmax. You should do that for this handin.

Applying Softmax
You will be testing your implementation on two data sets. The first is the wine data set that we have seen and tested on in week one. The second data set is the MNIST data set. It consists of images of hand-written digits. The goal is to predict/recognize the written digit, i.e. the labels are 0,1,...,9. The MNIST digits are represented as feature vectors with one coordinate per pixel in the image. The ordering of the pixels in the feature vector correspond to taking the pixels of the image and putting them row-by-row after each other.

Run python softmax_test.py -wine to test your implementation on the wine data set. The built in python implementation we tested in week one got above 90 percent test accuracy, so your implementation should so as well.

Run python softmax_test.py -show_digits show a small subset of the data set of MNIST digits, a data set for Optical Character Recognition

Run python softmax_test.py -digits to run you classifier on MNIST digits - the generated plot is automatically saved

Run python softmax_test.py -visualize to visualize the classifier trained on MNIST digits - the generated plot is automatically saved

You can tune the epochs, mini_batch_size, and initial learning rate from the command line i.e.

python softmax_test.py -show_digits -epochs 100 -lr 0.42 -bs 666

but the provided values should work well enough.

Report
Add a section "Part II: Softmax" with subsections "code" and "theory" to your report. In the "code" subsection you should do the same 2 points as you did for logistic regression.

Include the plots generated by softmax_test and remember to include the in sample and test accuracy achieved.

There is a single theory question specified in the next section.

Theoretical Question(s):
Assume that you use your softmax implementation on a problem with 
 classes with n,d, epochs and batch_size defined as for logistic_regression.

What is the running time of your softmax implementation i.e how long does your implementation of cost_grad take to compute the cost and the gradient.
Uploading to Brightspace
Make a zip archive of the two code files logistic_regression.py and softmax.py

Upload one pdf with the report to brightspace together with the zip file.

Ensure you upload the pdf separately!

Remember to put your names and student ids inside the pdf report!

The PDF should be at the most 5 pages!