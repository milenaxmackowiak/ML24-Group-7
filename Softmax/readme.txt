Softmax Regression
Implementing Softmax
In "softmax.py" you must complete the implementation of

softmax
predict
score
cost_grad
fit
The interface for each function is described in the file. All needed equations can be found in the slides and in the softmax notebook. Note that we have added a helper function def one_in_k_encoding(vec, k) that encodes a vector og length 
 of integer labels with class labels in 
 to a 
 matrix of one-in-k encoded labels.

You can test your implementation by running "python softmax.py". This is a small non-exhaustive test. You should consider writing your own test cases.

As for Logistic Regression, softmax sometimes suffers from numerical issues if we are not careful. Exponentation of large numbers and log of small numbers can lead to numerical issues. It is possible to implement the algorithm to be quite numerically stable if you use the trick provided in the Softmax note under the course reading material for Softmax. You should do that for this handin.

Applying Softmax
You will be testing your implementation on two data sets. The first is the wine data set that we have seen and tested on in week one. The second data set is the MNIST data set. It consists of images of hand-written digits. The goal is to predict/recognize the written digit, i.e. the labels are 0,1,...,9. The MNIST digits are represented as feature vectors with one coordinate per pixel in the image. The ordering of the pixels in the feature vector correspond to taking the pixels of the image and putting them row-by-row after each other.

Run python softmax_test.py -wine to test your implementation on the wine data set. The built in python implementation we tested in week one got above 90 percent test accuracy, so your implementation should so as well.

Run python softmax_test.py -show_digits show a small subset of the data set of MNIST digits, a data set for Optical Character Recognition

Run python softmax_test.py -digits to run you classifier on MNIST digits - the generated plot is automatically saved

Run python softmax_test.py -visualize to visualize the classifier trained on MNIST digits - the generated plot is automatically saved

You can tune the epochs, mini_batch_size, and initial learning rate from the command line i.e.

python softmax_test.py -show_digits -epochs 100 -lr 0.42 -bs 666

but the provided values should work well enough.

Report
Add a section "Part II: Softmax" with subsections "code" and "theory" to your report. In the "code" subsection you should do the same 2 points as you did for logistic regression.

Include the plots generated by softmax_test and remember to include the in sample and test accuracy achieved.

There is a single theory question specified in the next section.

Theoretical Question(s):
Assume that you use your softmax implementation on a problem with 
 classes with n,d, epochs and batch_size defined as for logistic_regression.

What is the running time of your softmax implementation i.e how long does your implementation of cost_grad take to compute the cost and the gradient.



*************Uploading to Brightspace*****************
Make a zip archive of the two code files logistic_regression.py and softmax.py

Upload one pdf with the report to brightspace together with the zip file.

Ensure you upload the pdf separately!

Remember to put your names and student ids inside the pdf report!

The PDF should be at the most 5 pages!